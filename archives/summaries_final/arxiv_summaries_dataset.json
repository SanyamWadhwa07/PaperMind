[
  {
    "overall_summary": "We introduce LARM, a feedforward model for 3D articulated object reconstruction. Beyond novel view and state synthesis, we perform this reconstruction process separately for the movable part and the main body, resulting in two disjoint meshes that can articulate independently based on the estimated joint parameters. Our method recovers a single set of canonical meshes and estimates joint parameters by leveraging multi-state observations. To obtain meshes under a specific articulation state, we simply transform the canonical meshes using the inferred joint parameters instead of reconstructing meshes again. To estimate explicit joint parameters using the LARM model, we first synthesize numerous image pairs with similar camera poses but different joint states. Next, we establish 2D pixel-wise corre- spondences for. Number of Views for Mesh Reconstruction. While we observe a slight drop in performance, the results remain reasonable. To extract joint parameters (e. g. , joint axis, pivot point), we query LARM at multiple joint states to construct correspondences and solve for the joint parameters. A greater number of joint states typically leads to more reliable dependences and, consequently, more accurate joint estimation. In row (d), we reduce the default 5 sampled joints states to 2 and ob- serve a drop in Performance, highlighting the importance of LARM\u2019s ability to synthesize outputs at novel joint states. Articulate-Anything and Singapo achieve better results by retrieving part meshes from the PartNetMobility dataset. However, due to the.",
    "section_summaries": {
      "methodology": "We introduce LARM, a feedforward model for 3D articulated object reconstruction. Beyond novel view and state synthesis, we perform this reconstruction process separately for the movable part and the main body, resulting in two disjoint meshes that can articulate independently based on the estimated joint parameters. Our method recovers a single set of canonical meshes and estimates joint parameters by leveraging multi-state observations. To obtain meshes under a specific articulation state, we simply transform the canonical meshes using the inferred joint parameters instead of reconstructing meshes again. To estimate explicit joint parameters using the LARM model, we first synthesize numerous image pairs with similar camera poses but different joint states. Next, we establish 2D pixel-wise corre- spondences for.",
      "discussion": "Number of Views for Mesh Reconstruction. While we observe a slight drop in performance, the results remain reasonable. To extract joint parameters (e. g. , joint axis, pivot point), we query LARM at multiple joint states to construct correspondences and solve for the joint parameters. A greater number of joint states typically leads to more reliable dependences and, consequently, more accurate joint estimation. In row (d), we reduce the default 5 sampled joints states to 2 and ob- serve a drop in Performance, highlighting the importance of LARM\u2019s ability to synthesize outputs at novel joint states. Articulate-Anything and Singapo achieve better results by retrieving part meshes from the PartNetMobility dataset. However, due to the."
    },
    "section_keywords": {
      "methodology": [
        "articulated",
        "poses",
        "reconstructing",
        "3d",
        "joint"
      ],
      "discussion": [
        "mesh",
        "meshes",
        "joints",
        "joint",
        "reconstruction"
      ]
    },
    "overall_keywords": [
      "articulated",
      "poses",
      "reconstructing",
      "3d",
      "mesh",
      "camera",
      "meshes",
      "joint",
      "joints",
      "reconstruction"
    ],
    "entities": {
      "models": [
        "CLIP",
        "ChatGPT",
        "GPT-4o",
        "Gemini"
      ],
      "metrics": [
        "accuracy",
        "precision",
        "cross-entropy",
        "Accuracy",
        "mIoU"
      ],
      "datasets": [],
      "frameworks": [],
      "techniques": []
    },
    "methodology_flowchart": "graph TD\n    Start([Start])\n    S1[\"We introduce LARM, a feedforward model for 3D articulated object reconstruction.\"]\n    S2[\"Beyond novel view and state synthesis, LARM also supports explicit 3D mesh reconstruction.\"]\n    S3[\"However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-inst...\"]\n    S4[\"Re- cent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstru...\"]\n    S5[\"In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extractio...\"]\n    End([End])\n    Start --> S1\n    S1 --> S2\n    S2 --> S3\n    S3 --> S4\n    S4 --> S5\n    S5 --> End\n",
    "sections_found": [
      "methodology",
      "discussion"
    ],
    "num_words_original": 9560,
    "num_words_summary": 232,
    "title": "LARM: A Large Articulated-Object Reconstruction Model",
    "authors": [
      "Sylvia Yuan",
      "Ruoxi Shi",
      "Xinyue Wei",
      "Xiaoshuai Zhang",
      "Hao Su",
      "Minghua Liu"
    ],
    "arxiv_id": "2511.11563v1",
    "published": "2025-11-14 18:55:27+00:00",
    "primary_category": "cs.CV",
    "abstract_original": "Modeling 3D articulated objects with realistic geometry, textures, and kinematics is essential for a wide range of applications. However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-instance optimization, limiting their scalability. Recent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstruction, and rely on brittle, complex multi-stage pipelines. We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures. LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis. In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extraction and joint estimation. Our pipeline eliminates the need for dense supervision and supports high-fidelity reconstruction across diverse object categories. Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images. project page: https://sylviayuan-sy.github.io/larm-site/"
  }
]