{
  "overall_summary": "With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemiporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotEMporal model. To handle the issue mentioned above, efficient AP methods are developed to reduce and redundancy in DNN models ,. , which integrates convolutional operations into the LSTM architecture. ConvLSTM models are computationally expensive, as they integrate convolutionAL operations into recurrent architectures. Instead of model compression, we aim to trade the redundant information in the input data or features for com- put. Training a deep ConvLSTM model requires a massive number of MAC operations in each layer, both in forward and backward propagation. Another approach is to reduce MAC operations by exploiting the intrinsic spatial and temporal sparsity of sequential data. proposed an iterative process of pruning and retraining. In each itera- tion, the model is first trained to learn informative weights. Then, unimportant weights with small magnitudes are pruned. Finally, the pruning model is retrained to fine-tune the pruned weights and regain model performance. To effectively compress CNN layers, Li et al. Pruning is also explored in RNN models. This technique does not require additional retraining steps used in. However, it introduced an. video sequences con- taining both real-world data acquired by live video recordings and simulated data generated by SolidWorks, a professional 3D modeling software. To enhance the visibility of temporal dynamics between adjacent frames and considering the computational resource constraints, we sampled each sequence every 5 frames and downsampled the size of each frame to 3 \u00d7 64 \u00d764. shows several samples of frames in both training and testing sequences. GFLOPs is reported as the average number of FLOPs over the testing dataset during inference. Our proposed SparseST model only introduces delta thresholds for each unit as additional parameters, which is negligible. It aims to fully exploit the spatial and temporal spar- sity to reduce the computational complexity. In our proposed model. Each point is annotated by the value of its associated preference weight wMSE. Ideally, all of these points are supposed to be Pareto optimal (non- dominated). However, we obtain these points by training a neural network, which means the convergence and global optimality of the training process cannot be fully guaranteed. And this is what standard linear scalarization fails to guarantee. A clear trade-off trend between the two objectives shows up along with the PAREto fronts. As the preference weight WMSE increases, the model achieves lower MSE (better accuracy) at the cost of higher occupancy (lower acceleration). Conversely, decreasing wM SE will favor sparsity and computational effi- ciency, but reduce predictive. One of our tasks is to perform a spatiotemporal prediction task on the Moving MNIST dataset. Case Study: Spatiotemscale Prediction : A sample of the Moving MinnesotaIST dataset, 1) Dataset: The Moving MNist dataset is a synthetic spatiotepporal benchmark dataset, which contains sequences of moving handwritten digits. It is widely used for evaluating sequence modeling and video prediction models. shows a sample of a sample. 2) Experimental Settings: Our task is next-frame spatiotEM- poral prediction with an autoencoder structure supervisedly. This task is based on a single-frame model of a moving handwritten digit.",
  "section_summaries": {
    "introduction": "With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemiporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotEMporal model. To handle the issue mentioned above, efficient AP methods are developed to reduce and redundancy in DNN models ,. , which integrates convolutional operations into the LSTM architecture. ConvLSTM models are computationally expensive, as they integrate convolutionAL operations into recurrent architectures. Instead of model compression, we aim to trade the redundant information in the input data or features for com- put.",
    "related_work": "Training a deep ConvLSTM model requires a massive number of MAC operations in each layer, both in forward and backward propagation. Another approach is to reduce MAC operations by exploiting the intrinsic spatial and temporal sparsity of sequential data. proposed an iterative process of pruning and retraining. In each itera- tion, the model is first trained to learn informative weights. Then, unimportant weights with small magnitudes are pruned. Finally, the pruning model is retrained to fine-tune the pruned weights and regain model performance. To effectively compress CNN layers, Li et al. Pruning is also explored in RNN models. This technique does not require additional retraining steps used in. However, it introduced an.",
    "methodology": "video sequences con- taining both real-world data acquired by live video recordings and simulated data generated by SolidWorks, a professional 3D modeling software. To enhance the visibility of temporal dynamics between adjacent frames and considering the computational resource constraints, we sampled each sequence every 5 frames and downsampled the size of each frame to 3 \u00d7 64 \u00d764. shows several samples of frames in both training and testing sequences. GFLOPs is reported as the average number of FLOPs over the testing dataset during inference. Our proposed SparseST model only introduces delta thresholds for each unit as additional parameters, which is negligible. It aims to fully exploit the spatial and temporal spar- sity to reduce the computational complexity. In our proposed model.",
    "discussion": "Each point is annotated by the value of its associated preference weight wMSE. Ideally, all of these points are supposed to be Pareto optimal (non- dominated). However, we obtain these points by training a neural network, which means the convergence and global optimality of the training process cannot be fully guaranteed. And this is what standard linear scalarization fails to guarantee. A clear trade-off trend between the two objectives shows up along with the PAREto fronts. As the preference weight WMSE increases, the model achieves lower MSE (better accuracy) at the cost of higher occupancy (lower acceleration). Conversely, decreasing wM SE will favor sparsity and computational effi- ciency, but reduce predictive.",
    "experiments": "One of our tasks is to perform a spatiotemporal prediction task on the Moving MNIST dataset. Case Study: Spatiotemscale Prediction : A sample of the Moving MinnesotaIST dataset, 1) Dataset: The Moving MNist dataset is a synthetic spatiotepporal benchmark dataset, which contains sequences of moving handwritten digits. It is widely used for evaluating sequence modeling and video prediction models. shows a sample of a sample. 2) Experimental Settings: Our task is next-frame spatiotEM- poral prediction with an autoencoder structure supervisedly. This task is based on a single-frame model of a moving handwritten digit."
  },
  "section_keywords": {
    "introduction": [
      "sparsest",
      "lstm",
      "edge",
      "sparsity",
      "compression"
    ],
    "related_work": [
      "rnn",
      "cnn",
      "pruning",
      "pruned",
      "retraining"
    ],
    "methodology": [
      "frames",
      "sparsest",
      "3d",
      "recordings",
      "frame"
    ],
    "discussion": [
      "optimality",
      "optimal",
      "preference",
      "pareto",
      "neural"
    ],
    "experiments": [
      "spatiotemscale",
      "spatiotemporal",
      "spatiotepporal",
      "spatiotem",
      "autoencoder"
    ]
  },
  "overall_keywords": [
    "sparsest",
    "cnn",
    "lstm",
    "autoencoder",
    "edge",
    "compression",
    "sparsity",
    "spatiotemporal",
    "pruned",
    "neural"
  ],
  "entities": {
    "models": [
      "LSTM",
      "GRU"
    ],
    "datasets": [
      "MNIST"
    ],
    "metrics": [
      "accuracy",
      "AUC",
      "precision",
      "ROC"
    ],
    "frameworks": [
      "PyTorch"
    ],
    "techniques": []
  },
  "methodology_flowchart": "graph TD\n    Start([Start])\n    S1[\"We describe in detail the surrogate model used in Section IV-E.\"]\n    S2[\"Each of the three gates it, ft, ot at each time step, with a range of values between 0 and 1, acts as a valve to control...\"]\n    S3[\"In addition, we give the computational cost analysis of the proposed model.\"]\n    End([End])\n    Start --> S1\n    S1 --> S2\n    S2 --> S3\n    S3 --> End\n",
  "sections_found": [
    "introduction",
    "related_work",
    "methodology",
    "discussion",
    "experiments"
  ],
  "num_words_original": 9105,
  "num_words_summary": 593,
  "title": "SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction",
  "authors": [
    "Junfeng Wu",
    "Hadjer Benmeziane",
    "Kaoutar El Maghraoui",
    "Liu Liu",
    "Yinan Wang"
  ],
  "arxiv_id": "2511.14753v1",
  "published": "2025-11-18 18:53:37+00:00",
  "primary_category": "cs.LG",
  "abstract_original": "",
  "pdf_path": "arxiv_papers\\2511.14753v1.pdf"
}